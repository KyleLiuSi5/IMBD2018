{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the IPython extention autoreload.\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading edited module without restarting the kernel.\n",
    "# Referring to https://support.enthought.com/hc/en-us/articles/204469240-Jupyter-IPython-After-editing-a-module-changes-are-not-effective-without-kernel-restart\n",
    "%autoreload\n",
    "\n",
    "from utils import *\n",
    "\n",
    "path_to_train_and_dev = '../../data/first-round/train_and_dev_set'\n",
    "path_to_test = '../../data/first-round/test_set'\n",
    "ratio_of_training_data = 0.8\n",
    "train_set_x, train_set_y, dev_set_x, dev_set_y, test_set_x = load_data(path_to_train_and_dev, ratio_of_training_data, path_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train x: 32 train y 32 dev x: 8 dev y: 8 test x: 10\n"
     ]
    }
   ],
   "source": [
    "print('train x:', len(train_set_x), 'train y', len(train_set_y), 'dev x:', len(\n",
    "    dev_set_x), 'dev y:', len(dev_set_y), 'test x:', len(test_set_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wuyihung\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 7500, 4)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7500, 4)           16        \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 7500, 64)          9472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7500, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7500, 64)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 34,897\n",
      "Trainable params: 34,633\n",
      "Non-trainable params: 264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "from model import *\n",
    "\n",
    "# Creating model.\n",
    "model = create_model(input_shape=(train_set_x.shape[1:]), summary=True)\n",
    "\n",
    "# Instantiating the optimizer.\n",
    "opt = Adam()\n",
    "\n",
    "# Determining the loss and metric, and compiling the model.\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading weights, if needed.\n",
    "#model.load_weights(\"./saved_weights/model-019epochs-0.028val_loss.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"./saved_weights/weights-{epoch:003d}epochs-{loss:.3f}loss-{val_loss:.3f}val_loss.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, save_weights_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#History_fit = model.fit(train_set_x, train_set_y, batch_size=train_set_x.shape[0], epochs=50, callbacks=callbacks_list, validation_data=(dev_set_x, dev_set_y))\n",
    "#history_fit = History_fit.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving history\n",
    "#import pickle\n",
    "#with open('./trainHistoryDict', 'wb') as file_pi:\n",
    "#    pickle.dump(history_fit, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the training and dev losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./trainHistoryDict', 'rb') as file_pi:\n",
    "    history_fit = pickle.load(file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating and printing root mean square error of fitting\n",
    "import numpy as np\n",
    "rms_train_w_dp_all = np.sqrt(history_fit['loss'])\n",
    "rms_dev_all = np.sqrt(history_fit['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rms_train_w_dp_all)\n",
    "plt.plot(rms_dev_all)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 137ms/step\n",
      "RMS error of training set of the best fitting: 0.21791957006441026\n",
      "RMS error of dev set of the best fitting: 0.16839495634703022\n"
     ]
    }
   ],
   "source": [
    "# Determining the best epoch.\n",
    "min_epoch = np.argmin(rms_dev_all)\n",
    "\n",
    "# Loading the corresponding model.\n",
    "model.load_weights(\"./saved_weights/weights-003epochs-0.304loss-0.028val_loss.h5\")\n",
    "\n",
    "# Evaluating training set without dropout layer.\n",
    "ms_train_wo_dp_best = model.evaluate(x=train_set_x, y=train_set_y, batch_size=train_set_x.shape[0])\n",
    "rms_train_wo_dp_best = np.sqrt(ms_train_wo_dp_best)\n",
    "\n",
    "print('RMS error of training set of the best fitting:', rms_train_wo_dp_best)\n",
    "print('RMS error of dev set of the best fitting:', rms_dev_all[min_epoch])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
